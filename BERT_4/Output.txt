C:\Users\Sepehr\anaconda3\envs\py310\python.exe C:\ITU\RFI-proje\BERT_modeling\BERT-V.1.4.py 
2024-10-30 20:11:59.048216: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-30 20:11:59.596287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5450 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6

Sample X_train after split:
['im a huge proponent of assassins creed translates to i am a ubisoft quebec player'
 'heres hoping for a prepatch announcement tomorrow'
 'nah this is the hours where you google anything or watch weird shit on youtube why am i watching videos about the dank web lmfaoo'
 'of all the carts that have a kerb verizon is the only one that doesnt and i probably have a problem with my phone'
 'had no issues all week playing ys origins  fairy tail or even assassins creed odyssey  load up forza and i have  disconnects completely where my modem has to reset it self  wtf is this shit']

Sample tokenized input IDs (train):
[[  101 10047  1037  4121 22488  1997 18364 16438 16315  2000  1045  2572
   1037  1057 18477 15794  5447  2447   102     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101  2182  2015  5327  2005  1037 17463  4017  2818  8874  4826   102
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]

Sample tokenized attention masks (train):
[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
Train Input IDs shape: (55292, 128)
Train Attention Masks shape: (55292, 128)
Validation Input IDs shape: (974, 128)
Test Input IDs shape: (13824, 128)
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFBertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_ids (InputLayer)         [(None, 128)]        0           []                               
                                                                                                  
 attention_masks (InputLayer)   [(None, 128)]        0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              
                                thPoolingAndCrossAt               'attention_masks[0][0]']        
                                tentions(last_hidde                                               
                                n_state=(None, 128,                                               
                                 768),                                                            
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dense (Dense)                  (None, 4)            3076        ['tf_bert_model[0][1]']          
                                                                                                  
==================================================================================================
Total params: 109,485,316
Trainable params: 109,485,316
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/5
2024-10-30 20:12:29.727885: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
3456/3456 [==============================] - 694s 197ms/step - loss: 0.8632 - accuracy: 0.6514 - val_loss: 0.4527 - val_accuracy: 0.8316
Epoch 2/5
3456/3456 [==============================] - 665s 193ms/step - loss: 0.4180 - accuracy: 0.8502 - val_loss: 0.1930 - val_accuracy: 0.9446
Epoch 3/5
3456/3456 [==============================] - 673s 195ms/step - loss: 0.1843 - accuracy: 0.9362 - val_loss: 0.1637 - val_accuracy: 0.9600
Epoch 4/5
3456/3456 [==============================] - 670s 194ms/step - loss: 0.1004 - accuracy: 0.9650 - val_loss: 0.1918 - val_accuracy: 0.9517
Epoch 5/5
3456/3456 [==============================] - 652s 189ms/step - loss: 0.0723 - accuracy: 0.9748 - val_loss: 0.2028 - val_accuracy: 0.9589
432/432 [==============================] - 43s 96ms/step
	Classification Report for BERT:

               precision    recall  f1-score   support

    Negative       0.93      0.94      0.93      4195
     Neutral       0.89      0.94      0.91      3420
    Positive       0.92      0.93      0.93      3777
  Irrelevant       0.96      0.85      0.90      2432

    accuracy                           0.92     13824
   macro avg       0.93      0.92      0.92     13824
weighted avg       0.92      0.92      0.92     13824

432/432 [==============================] - 50s 116ms/step - loss: 0.3036 - accuracy: 0.9212
Test Loss: 0.30356350541114807, Test Accuracy: 0.9212239384651184

Process finished with exit code 0
