C:\Users\Sepehr\anaconda3\envs\py310\python.exe C:\ITU\RFI-proje\Model-training\BERT-3\BERT_V.1.3.py 
2024-11-02 17:01:21.633687: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-02 17:01:22.801943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5450 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6

Sample X_train after split:
['amazon great indian sale get rs  amazon pay cashback with a new oneplus tv read more at ourswidewebcom    tco  ipktvwuyq'
 'ubisoftsupport rainbowgame would u guys please for the sake of god shut down xresolver im sick of constantly hit off'
 'ghostrecon how do we fix the cursor issue nobody really remember the last update with this issue'
 'eamaddennfl the disrespect to baltimore is unreal  most pro bowlers on our team everthe mvp isnt even a  the best kicker ever isnt even a  and the highest rated player on our team is an offseason acquisition laughable'
 'all most got that ace in ranked']

Sample tokenized input IDs (train):
[[  101  9733  2307  2796  5096  2131 12667  9733  3477  5356  5963  2007
   1037  2047  2028 24759  2271  2694  3191  2062  2012 14635 22517  8545
   9818  5358 22975  2080 12997 25509  2615 16050  2100  4160   102     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101  1057 18477 15794  6342  9397 11589 10098 16650  2052  1057  4364
   3531  2005  1996  8739  1997  2643  3844  2091  1060  6072  4747  6299
  10047  5305  1997  7887  2718  2125   102     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]

Sample tokenized attention masks (train):
[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
Train Input IDs shape: (48381, 128)
Train Attention Masks shape: (48381, 128)
Validation Input IDs shape: (974, 128)
Test Input IDs shape: (20735, 128)

Sample one-hot encoded labels (train):
[[0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]]
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFBertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_ids (InputLayer)         [(None, 128)]        0           []                               
                                                                                                  
 attention_masks (InputLayer)   [(None, 128)]        0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              
                                thPoolingAndCrossAt               'attention_masks[0][0]']        
                                tentions(last_hidde                                               
                                n_state=(None, 128,                                               
                                 768),                                                            
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 dense (Dense)                  (None, 4)            3076        ['tf_bert_model[0][1]']          
                                                                                                  
==================================================================================================
Total params: 109,485,316
Trainable params: 109,485,316
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/5
2024-11-02 17:01:59.452185: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
3024/3024 [==============================] - 665s 215ms/step - loss: 0.8866 - accuracy: 0.6438 - val_loss: 0.5273 - val_accuracy: 0.7906
Epoch 2/5
3024/3024 [==============================] - 721s 238ms/step - loss: 0.4608 - accuracy: 0.8337 - val_loss: 0.1946 - val_accuracy: 0.9292
Epoch 3/5
3024/3024 [==============================] - 570s 188ms/step - loss: 0.2086 - accuracy: 0.9267 - val_loss: 0.1368 - val_accuracy: 0.9589
Epoch 4/5
3024/3024 [==============================] - 569s 188ms/step - loss: 0.1120 - accuracy: 0.9608 - val_loss: 0.1803 - val_accuracy: 0.9528
Epoch 5/5
3024/3024 [==============================] - 568s 188ms/step - loss: 0.0735 - accuracy: 0.9738 - val_loss: 0.2247 - val_accuracy: 0.9538
648/648 [==============================] - 74s 111ms/step
	Classification Report for BERT:

               precision    recall  f1-score   support

    Negative       0.95      0.90      0.93      6293
     Neutral       0.85      0.95      0.90      5129
    Positive       0.93      0.91      0.92      5665
  Irrelevant       0.92      0.89      0.90      3648

   micro avg       0.91      0.91      0.91     20735
   macro avg       0.91      0.91      0.91     20735
weighted avg       0.91      0.91      0.91     20735
 samples avg       0.91      0.91      0.91     20735

648/648 [==============================] - 73s 113ms/step - loss: 0.3494 - accuracy: 0.9119
Test Loss: 0.34940338134765625, Test Accuracy: 0.9118881225585938

Process finished with exit code 0
